---
title: "Latent Dirichlet Allocation for Metagenomics: testing an initial assumption"
author: "Russell Dinnage"
date: "23/07/2014"
output: 
  html_document:
    highlight: pygments
  
---

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)

## Global options
options(max.print="75")
opts_chunk$set(fig.path="out/",
               echo=TRUE,
         cache=TRUE,
               cache.path="cache/",
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

What I want to do here is find out if Latent Dirichlet Allocation and similar topic models are suitable for metagenomics data. I want to test if the probability of a k-mer (or genomic 'word'), or it's frequency in a genome can be approximated by the frequency of that k-mer in a collection of short reads drawn from that genome. My intuition is that it can, but the quality of the approximation will depend of the read length and the chromosome length of the genome. This is because overlapping regions of the reads could mess up the approximation, if they are not distibuted uniformly throughout the genome. I believe they should be for the most part, but this could break down near the ends of chromosomes (e.g. reads that are too close to the ends will be tossed because they will be too short, and so the reads just to the other side will have less overlaps, meaning the kmers here may be underrepresented). Since the length of genome that are a read's length from the ends of chromosome are probably rare compared to the full length of the genome, I suspect this won't be a big issue.

If we can approximate the k-mer frequencies of a source genome from its reads, then we can model a mixture of reads from many genomes as a mixture model where the frequency of a k-mer is equal to the sum of the frequencies of that k-mer in each genome times the frequency of each genome in that particular mixture. This is exactly what is modelled with a Latent Dirichlet Allocation model, only it is usually used to model documents as a mixture of topics (e.g. probability of a word in a document is equal to probability in each topic, times the probability of each topic being in that document). Documents are sites, topics are genomes, and words are k-mers.

Load the necessary project package:
```{r loads}
if (!require("LDAmetagenomics", character.only = TRUE)) devtools::install_github("rdinnager/LDA-metagenomics")
library(LDAmetagenomics)
set.seed(20140728)
```

First I am going to simulate a set of genomes using the software ALF (through my package `Ralf`):

```{r gen_genomes, cache = TRUE, message=FALSE}
library(magrittr)
if (!require("LDAmetagenomics", character.only = TRUE)) devtools::install_github("rdinnager/Ralf")
library("Ralf")
## generate 50 genomes of 50 genes each
if (!file.exists("temp/genome_sim")){
  genome_sim <- run_ALF("genome_sim", 50, 50, 400, 50, 0.06, 0.025, 0.0001, "temp", 
                        "/home/din02g/ALF_standalone/bin", TRUE) %>% load_ALF
} else {
  genome_sim <- "temp/genome_sim" %>% load_ALF  
}
```

Let's take a look at the simulated tree used to simulate those genomes, to see if everything looks okay.

```{r tree_plot, cache=TRUE, dev='svg'}
plot(genome_sim$tree, cex = 0.65, no.margin = TRUE)
```

The next step it to concatenate the 50 independently simulated genes into one long genomic sequence. We will assume this constitutes a single genome unit, such as a chromosome.

```{r dna_convert, cache=TRUE}
genome_chrom <- ALF_cat(genome_sim)
## convert dna to DNAStringSet for analysis with Biostrings
genome_dna <- DNAStringSet(sapply(genome_chrom$dna, function(x) unlist(x)))
genome_dna
```

Okay, now that we have about 50 genomes to play with, we are going to start with just one. We will simulate drawing reads from this genome. For the moment, we will assume that the reads have no errors in them, and they are all 90 base-pairs long (approximately the read-length of Illumina NGS). This uses the `gen_segs()` function from the project package.

```{r make_reads}
working_seq <- genome_dna %>% sample(1)
reads <- gen_segs(working_seq, len = 90, num = 1000)
reads
```

We can count the frequency of different genomic words of a given length $K$, using the `oligonucleotideFrequency()` function in `Biostrings`. To keep memory usage down, we will only look at small $K$ to begin with. To look at bigger $K$ we will have to come up with something more memory efficient than storing the frequency of all _possible_ words!

```{r kmer_freq}
genome_kmer_counts <- oligonucleotideFrequency(working_seq, width = 5, as.prob = TRUE)
genome_kmer_counts[ , 50:60]
## the frequencies in the short reads
reads_kmer_counts <- oligonucleotideFrequency(reads, width = 5, as.prob = TRUE)
head(reads_kmer_counts[ , 50:60])
```

The expected frequency of a k-mer in the genome should be approximated by the mean frequency of the k-mer in the short reads drawn from the genome. Let's see if this holds up.

```{r test_kmers, dev='svg'}
reads_kmers <- apply(reads_kmer_counts, 2, mean)
library(ggplot2)
kmer_dat <- data.frame(Genome = as.vector(genome_kmer_counts), Reads = reads_kmers) %>% tbl_df
kmer_dat
p <- ggplot(kmer_dat, aes(Genome, Reads)) + geom_point() + stat_function(fun = function(x) x, colour = "white")
p
```

The white line is the 1:1 line, so it looks like the mean frequency of a k-mer in the reads is not a bad (unbiased) estimator of its frequency in the source genome. There is some error though. Let's see if we can reduce the error by increasing the number of reads (in other words, increase the read "coverage").

```{r test_more_reads, dev='svg'}
reads10000 <- gen_segs(working_seq, len = 90, num = 10000)
reads_kmer_counts10000 <- oligonucleotideFrequency(reads10000, width = 5, as.prob = TRUE)
reads_kmers10000 <- apply(reads_kmer_counts10000, 2, mean)
kmer_dat10000 <- data.frame(Genome = as.vector(genome_kmer_counts), Reads = reads_kmers10000) %>% tbl_df
p <- ggplot(kmer_dat10000, aes(Genome, Reads)) + geom_point() + stat_function(fun = function(x) x, colour = "white")
p
```

Indeed, this looks much better. There is probably a way to analytically determine the error distribution as a function of number of reads and read length, but that is beyond my meager math skills at the moment. This would be very useful down the line in this project, so I will do some thinking on it. If anyone happens to be reading this, and know of any mathematical work on this type of question, please let me know!

The next step will be to simulate "communities" of genomes by shuffling genomes together into groups. Then for each group or community, simulate drawing short reads from the genome, where the chance of a read being drawn from a genome is proportional to its relative abundance in the sample. Then we can see if the frequency of k-mers in the reads effectively approximates the frequency we would expect based on the abundances of the source genomes in each community.
